---
title: "Ollama - Self-hosted Multi Language Model"
description: "Self-hosted LLM server with monitoring and networking study"
showDate: true
date: "2024-12-01"
showAuthor: true
showReadingTime: true
---

**Tech Stack:** Ubuntu Server, Cloudflare, Docker, Prometheus, Grafana

**Duration:** 12 months | **Team:** 1 member

Built a dual-PC self-hosted server setup for studying networking and hosting services. One low-energy PC hosts the website using Cloudflare tunnel with a personal domain, while a high-energy server handles compute-intensive tasks. All services including the website, Cloudflare tunnel, and large language models are Dockerized for efficient resource management and automatic restarts, with performance monitoring through Prometheus and log visualization via Grafana.